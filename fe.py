import streamlit as st
import yfinance as yf
import pandas as pd
import numpy as np
from tavily import TavilyClient
from openai import OpenAI
import plotly.graph_objects as go
import json
import concurrent.futures
import time
import random
import requests
import feedparser
import re

# --- HARDCODED KEYS (Hidden from UI) ---
TAVILY_API_KEY = "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"
ALPHA_VANTAGE_KEY = "8G1QKAWN221XEZR8"

# --- MODEL CONFIGURATION ---
MODELS = {
    "ROUTER": "Qwen/Qwen2.5-72B-Instruct",
    "NEWS": "MiniMaxAI/MiniMax-M2",
    "LOGIC": "deepseek-ai/DeepSeek-V3",
    "THINKING": "moonshotai/Kimi-K2-Thinking" 
}

SPECIFIC_MODELS = {
    "DEEPSEEK": "deepseek-ai/DeepSeek-V3", 
    "KIMI": "moonshotai/Kimi-K2-Thinking",
    "MINIMAX": "MiniMaxAI/MiniMax-M2",
    "QWEN": "Qwen/Qwen2.5-72B-Instruct"
}

# --- PAGE SETUP ---
st.set_page_config(
    page_title="MAS è”åˆç ”æŠ¥ç»ˆç«¯ v3.0",
    page_icon="ğŸ¦",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .stApp { background-color: #ffffff; color: #1f2937; }
    .stTextInput > div > div > input { background-color: #f3f4f6; color: #1f2937; }
    .stChatMessage .stChatMessageAvatar { background-color: #e5e7eb; border-radius: 50%; }
    div[data-testid="metric-container"] { background-color: #f9fafb; border: 1px solid #e5e7eb; padding: 10px; border-radius: 8px; }
    
    /* Thinking Process Style */
    .thinking-box {
        font-size: 0.85em;
        color: #6b7280;
        border-left: 3px solid #e5e7eb;
        padding-left: 10px;
        margin-bottom: 10px;
        font-style: italic;
    }
</style>
""", unsafe_allow_html=True)

# --- SIDEBAR ---
with st.sidebar:
    st.title("ğŸ›ï¸ æ§åˆ¶å°")
    st.subheader("ğŸ”‘ é‰´æƒè®¾ç½®")
    silicon_flow_key = st.text_input("è¯·è¾“å…¥ SiliconFlow API Key", type="password", help="ç”¨äºè°ƒç”¨æ¨¡å‹")
    if not silicon_flow_key:
        st.warning("âš ï¸ è¯·è¾“å…¥ API Key ä»¥å¯åŠ¨ç³»ç»Ÿ")
    st.divider()
    st.caption("Multi-Agent Research System v3.0\nPowered by SiliconFlow")

# --- BACKEND UTILS ---

def get_llm_client():
    if not silicon_flow_key: return None
    return OpenAI(api_key=silicon_flow_key, base_url="https://api.siliconflow.cn/v1")

def get_tavily_client():
    return TavilyClient(api_key=TAVILY_API_KEY)

def calculate_technical_indicators(df):
    if df.empty: return df
    exp12 = df['Close'].ewm(span=12, adjust=False).mean()
    exp26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp12 - exp26
    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['Signal_Line']
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    return df

def fetch_market_data(ticker):
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="6mo")
        if hist.empty: raise ValueError("Empty Data")
        hist = calculate_technical_indicators(hist)
        info = stock.info
        return {
            "status": "ONLINE (YF)",
            "symbol": ticker.upper(),
            "name": info.get('longName', ticker),
            "price": info.get('currentPrice', hist['Close'].iloc[-1]),
            "change_pct": ((hist['Close'].iloc[-1] - hist['Close'].iloc[-2])/hist['Close'].iloc[-2])*100,
            "pe": info.get('trailingPE', 'N/A'),
            "cap": info.get('marketCap', 'N/A'),
            "history_df": hist,
            "last_macd": {"hist": hist['MACD_Hist'].iloc[-1]},
            "last_rsi": hist['RSI'].iloc[-1]
        }
    except:
        return {"status": "OFFLINE", "error": "Market data unavailable"}

def search_web(query, topic="general"):
    """Broad search strategy."""
    try:
        tavily = get_tavily_client()
        res = tavily.search(query=query, topic=topic, max_results=5)
        return [f"- {r['title']}: {r['content'][:300]}" for r in res['results']]
    except:
        return ["æš‚æ— ç›¸å…³ç½‘ç»œæœç´¢æ•°æ®"]

def call_agent(agent_name, model_id, system_prompt, user_prompt, thinking_needed=False):
    client = get_llm_client()
    if not client: return "è¯·é…ç½® API Key", ""
    
    final_sys_prompt = system_prompt
    if thinking_needed:
        final_sys_prompt += "\nIMPORTANT: You MUST first output your internal thinking process wrapped in <thinking>...</thinking> tags, then output your final response."

    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": final_sys_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=2048 # Increased for thinking
        )
        content = response.choices[0].message.content
        
        # Parse Thinking
        thinking = ""
        if "<thinking>" in content and "</thinking>" in content:
            match = re.search(r"<thinking>(.*?)</thinking>", content, re.DOTALL)
            if match:
                thinking = match.group(1).strip()
                content = re.sub(r"<thinking>.*?</thinking>", "", content, flags=re.DOTALL).strip()
        
        return content, thinking
    except Exception as e:
        return f"âš ï¸ {agent_name} Error: {str(e)}", ""

# --- MAIN LOGIC ---

st.title("ğŸ¦ MAS è”åˆç ”æŠ¥ç»ˆç«¯ v3.0")
st.caption(f"æ··åˆæ¨¡å‹å¼•æ“: Qwen (è·¯ç”±) | MiniMax (æƒ…æŠ¥) | DeepSeek (åˆ†æ) | Kimi (é¦–å¸­ç ”ç©¶)")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "é¦–å¸­ç ”ç©¶å‘˜å°±ä½ã€‚è¯·ä¸‹è¾¾è°ƒç ”æŒ‡ä»¤ï¼ˆå¦‚ï¼šåˆ†æ ç‰¹æ–¯æ‹‰ï¼‰ã€‚", "avatar": "ğŸ‘¨â€ğŸ”¬"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"], avatar=msg.get("avatar")):
        st.markdown(msg["content"])
        if msg.get("thinking"):
            with st.expander("ğŸ§  æ€è€ƒè¿‡ç¨‹ (Thinking Chain)", expanded=False):
                st.markdown(f"_{msg['thinking']}_")

if user_input := st.chat_input("è¯·è¾“å…¥æ ‡çš„..."):
    if not silicon_flow_key:
        st.error("è¯·å…ˆé…ç½® SiliconFlow Key")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": user_input, "avatar": "ğŸ‘¤"})
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(user_input)

    # 1. Router
    ticker = None
    with st.chat_message("assistant", avatar="ğŸ‘©â€ğŸ’¼"):
        st.write("ğŸ”„ è‘£ç§˜æ­£åœ¨ç«‹é¡¹...")
        res, _ = call_agent("Router", SPECIFIC_MODELS["QWEN"], "æå–Yahoo Ticker JSON {'ticker': '...'}", user_input)
        try:
            ticker = json.loads(res.replace("```json","").replace("```",""))['ticker']
            st.markdown(f"âœ… æ ‡çš„ç¡®è®¤ï¼š**{ticker}**")
        except:
            st.error("æ— æ³•è¯†åˆ«æ ‡çš„")
            st.stop()

    # 2. Data Fetching (Initial)
    mkt = fetch_market_data(ticker)
    if mkt['status'] == "OFFLINE":
        st.error("è¡Œæƒ…æ•°æ®è·å–å¤±è´¥")
        st.stop()

    queries = {
        "macro": "global macro economy news market trends",
        "meso": f"{ticker} industry competitors market share",
        "micro": f"{ticker} stock news financial reports analysis",
        "pol": "international geopolitics trade war impact"
    }
    
    with st.status("ğŸ“¡ æ­£åœ¨è¿›è¡Œå…¨ç½‘æƒ…æŠ¥æœé›†...", expanded=True) as status:
        raw_news = {}
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {k: executor.submit(search_web, v, "news" if k != "meso" else "general") for k, v in queries.items()}
            for k, f in futures.items():
                raw_news[k] = f.result()
        status.update(label="âœ… åˆå§‹æƒ…æŠ¥å·²å°±ç»ª", state="complete")

    # --- THE PROCESS LOOP (Support 1 Retry) ---
    max_retries = 1
    retry_count = 0
    final_report = ""
    
    while retry_count <= max_retries:
        
        # 3. Meeting (Intelligence Reporting)
        # We collect opinions first
        opinions = {}
        
        # Render Agent Avatars only if it's the first run or specifically requested
        if retry_count == 0:
            st.subheader("ğŸ—£ï¸ æŠ•ç ”æ™¨ä¼š (Morning Meeting)")
        else:
            st.subheader("ğŸ”„ è¡¥å……ç ”è®¨ (Follow-up Meeting)")

        # Macro
        with st.chat_message("assistant", avatar="ğŸŒ"):
            res, _ = call_agent("Macro", SPECIFIC_MODELS["MINIMAX"], "ä½ æ˜¯å®è§‚åˆ†æå¸ˆã€‚ç®€è¿°å®è§‚ç¯å¢ƒã€‚æœ‰ä»€ä¹ˆè¯´ä»€ä¹ˆï¼Œç¡®ä¿å‡†ç¡®ã€‚", str(raw_news['macro']))
            st.markdown(f"**å®è§‚**: {res}")
            opinions['macro'] = res

        # Meso
        with st.chat_message("assistant", avatar="ğŸ­"):
            res, _ = call_agent("Meso", SPECIFIC_MODELS["MINIMAX"], f"ä½ æ˜¯è¡Œä¸šåˆ†æå¸ˆã€‚{ticker} è¡Œä¸šæƒ…å†µå¦‚ä½•ï¼Ÿç›¸å…³æ€§ä½ä¹Ÿæ²¡å…³ç³»ï¼Œè¯´ä½ çŸ¥é“çš„ã€‚", str(raw_news['meso']))
            st.markdown(f"**è¡Œä¸š**: {res}")
            opinions['meso'] = res

        # Micro
        with st.chat_message("assistant", avatar="ğŸ”"):
            res, _ = call_agent("Micro", SPECIFIC_MODELS["MINIMAX"], f"ä½ æ˜¯ä¸ªè‚¡åˆ†æå¸ˆã€‚{ticker} æœ€è¿‘æœ‰ä»€ä¹ˆæ–°é—»ï¼Ÿ", str(raw_news['micro']))
            st.markdown(f"**ä¸ªè‚¡**: {res}")
            opinions['micro'] = res

        # Quant & Finance (Quick Check)
        with st.chat_message("assistant", avatar="ğŸ’¹"):
            quant_ctx = f"Price:{mkt['price']}, PE:{mkt['pe']}, RSI:{mkt['last_rsi']:.1f}"
            res, _ = call_agent("Finance", SPECIFIC_MODELS["DEEPSEEK"], "è¯„ä»·ä¼°å€¼ä¸æŠ€æœ¯é¢çŠ¶æ€ã€‚", quant_ctx)
            st.markdown(f"**é‡åŒ–è´¢ç»**: {res}")
            opinions['fin_quant'] = res

        # 4. Analyst Drafting
        with st.chat_message("assistant", avatar="ğŸ“"):
            st.write("âœï¸ ç»¼åˆåˆ†æå¸ˆæ­£åœ¨æ’°å†™è‰æ¡ˆ...")
            full_context = f"æƒ…æŠ¥:{json.dumps(opinions, ensure_ascii=False)}\nè¡Œæƒ…:{quant_ctx}"
            report_draft, _ = call_agent("Analyst", SPECIFIC_MODELS["DEEPSEEK"], 
                                "ä½ æ˜¯é¦–å¸­åˆ†æå¸ˆã€‚æ’°å†™ä¸€ä»½ç®€æ˜ç ”æŠ¥ï¼ŒåŒ…å«é€»è¾‘ã€é£é™©å’Œç»“è®ºã€‚", full_context)
            st.markdown(report_draft)

        # 5. Chief Researcher Review (Kimi-Thinking)
        with st.chat_message("assistant", avatar="ğŸ‘¨â€ğŸ”¬"):
            st.write("ğŸ•µï¸ **é¦–å¸­ç ”ç©¶å‘˜ (Kimi)** æ­£åœ¨æ·±åº¦è¯„ä¼°...")
            
            review_prompt = f"""
            ä½ æ˜¯é¦–å¸­ç ”ç©¶å‘˜ã€‚è¯·å®¡æŸ¥è¿™ä»½ç ”æŠ¥å’Œç°æœ‰æƒ…æŠ¥ã€‚
            
            1. å¦‚æœä½ è®¤ä¸ºæŸä¸ªé¢†åŸŸï¼ˆå®è§‚/è¡Œä¸š/ä¸ªè‚¡ï¼‰çš„ä¿¡æ¯ä¸¥é‡ç¼ºå¤±å¯¼è‡´æ— æ³•åˆ¤æ–­ï¼Œè¯·è¾“å‡ºæŒ‡ä»¤ï¼šREWORK: [FIELD] (ä¾‹å¦‚ REWORK: MACRO)ã€‚
            2. å¦‚æœä¿¡æ¯å……è¶³ï¼Œè¯·è¿›è¡Œæ·±åº¦æ€è€ƒï¼Œè¾“å‡ºæœ€ç»ˆæŠ•èµ„å»ºè®®ã€‚
            
            ç ”æŠ¥è‰æ¡ˆ:
            {report_draft}
            """
            
            review_res, thinking = call_agent("Chief", SPECIFIC_MODELS["KIMI"], review_prompt, "è¯·å¼€å§‹å®¡æ ¸ã€‚", thinking_needed=True)
            
            # Show Thinking
            if thinking:
                with st.expander("ğŸ§  é¦–å¸­çš„æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)", expanded=False):
                    st.markdown(f"_{thinking}_")
            
            # Check for Rework
            if "REWORK:" in review_res and retry_count < max_retries:
                # Extract field
                match = re.search(r"REWORK:\s*(\w+)", review_res)
                field = match.group(1).lower() if match else "general"
                
                st.warning(f"ğŸš¨ é¦–å¸­é©³å›ï¼šè®¤ä¸º {field} é¢†åŸŸä¿¡æ¯ä¸è¶³ï¼Œè¦æ±‚è¿”å·¥ï¼")
                st.markdown(f"_{review_res}_")
                
                # Action: Search again with broader query
                st.write(f"ğŸ” æ­£åœ¨é’ˆå¯¹ **{field}** è¿›è¡Œæ·±åº¦è¡¥å……æœç´¢...")
                new_query = f"{ticker} {field} deep analysis details"
                new_info = search_web(new_query, "general")
                
                # Update context
                if field in raw_news:
                    raw_news[field].extend(new_info)
                else:
                    raw_news['micro'].extend(new_info) # Fallback
                
                retry_count += 1
                time.sleep(1)
                st.rerun() # Rerun logic (simulate loop) - actually in this structure we just continue loop
                continue # Go to next iteration of while loop
                
            else:
                # Final Success
                st.success("âœ… å®¡æ ¸é€šè¿‡ï¼Œæœ€ç»ˆå‘å¸ƒã€‚")
                st.markdown(f"### ğŸ† é¦–å¸­æœ€ç»ˆå†³ç­–\n\n{review_res}")
                
                # Save to history
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"### ğŸ“‘ æœ€ç»ˆç ”æŠ¥\n\n{report_draft}\n\n---\n**ğŸ† é¦–å¸­ç‚¹è¯„**: {review_res}", 
                    "avatar": "ğŸ‘¨â€ğŸ”¬",
                    "thinking": thinking
                })
                break
